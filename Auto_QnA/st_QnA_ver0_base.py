import streamlit as st
from langchain.chat_models import ChatOpenAI

api_key = st.secrets["OPENAI_API_KEY"]

st.set_page_config(
    page_title="AI ê¸°ë°˜ ê³ ê° ì‘ëŒ€ ì„œë¹„ìŠ¤",
    page_icon="ğŸŒ",
    layout="wide"
)

st.markdown("""
    <div style="background-color:#4A90E2;padding:10px;border-radius:10px;">
        <h1 style="color:white;text-align:center;"> ğŸ¤– AI ê³ ê° ì‘ëŒ€ ì„œë¹„ìŠ¤</h1>
    </div>
""", unsafe_allow_html=True)

st.markdown("""
    <div style="padding: 10px;">
        <h3 style="color:#333; text-align:center;">AIì™€ì˜ ëŒ€í™”ë¥¼ í†µí•´ ê³ ê°ë‹˜ì˜ ê¶ê¸ˆì¦ì„ ì‹ ì†í•˜ê²Œ í•´ê²°í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.</h3>
        <p style="text-align:center;">ë¬¸ì˜ì‚¬í•­ì„ ì•„ë˜ì— ì…ë ¥í•˜ì‹œê³  <strong>ë³´ë‚´ê¸°</strong> ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!</p>
    </div>
""", unsafe_allow_html=True)

def generate_response(input_text): 
    llm = ChatOpenAI(
        openai_api_key=api_key,
        temperature=1.0,
        model_name='gpt-3.5-turbo'  # OpenAIì—ì„œ ì§€ì›í•˜ëŠ” ì‹¤ì œ ëª¨ë¸ëª…
    )
    response = llm.predict(input_text)
    st.markdown(f"""
        <div style="background-color:#F0F0F0;padding:15px;border-radius:10px;margin-top:15px;">
            <h4>ğŸ’¡ ê³ ê°ë‹˜ê»˜ ë“œë¦¬ëŠ” ë‹µë³€:</h4>
            <p>{response}</p>
        </div>
    """, unsafe_allow_html=True)

with st.form("question_form"):
    text = st.text_area(
        "ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:", 
        placeholder="ì˜ˆ: ë°˜í’ˆ ì •ì±…ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        height=150
    )
    submitted = st.form_submit_button("ë³´ë‚´ê¸°", use_container_width=True)

    if submitted:
        if text.strip():
            generate_response(text)
        else:
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

st.markdown("""
    <div style="margin-top: 50px; padding: 15px; background-color: #333; color: white; text-align: center; border-radius: 10px;">
        <p>Powered by LangChain & OpenAI API | ğŸ¦ ê³ ê°ì˜ ë§Œì¡±ì„ ìœ„í•´ ì–¸ì œë‚˜ ìµœì„ ì„ ë‹¤í•©ë‹ˆë‹¤.</p>
    </div>
""", unsafe_allow_html=True)
